{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxeICokW8xCX",
        "outputId": "b2ebb697-647a-4446-fd5b-a7023fea42f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.5)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.8)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UdHEVz-r82Zd"
      },
      "outputs": [],
      "source": [
        "#Dataset preparation\n",
        "#User Name: chizitaraigwe\n",
        "#Authentication key: 4b1a9ab784421ff19944bd8de86ae9b3\n",
        "\n",
        "import opendatasets as opd\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Il67u_c89okF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583964c9-83ae-463d-dac9-ed0c4e1ee87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: s\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/volkandl/car-brand-logos\n",
            "Downloading car-brand-logos.zip to ./car-brand-logos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 217M/217M [00:02<00:00, 92.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "opd.download(\"https://www.kaggle.com/datasets/volkandl/car-brand-logos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9yKWKYumJ3gE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6798e9a-2d55-462a-8478-33b121b57523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: \n",
            "Your Kaggle username: s\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/kaggleashwin/vehicle-type-recognition\n",
            "Downloading vehicle-type-recognition.zip to ./vehicle-type-recognition\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 159M/159M [00:02<00:00, 77.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "opd.download(\"https://www.kaggle.com/datasets/kaggleashwin/vehicle-type-recognition\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opd.download(\"https://www.kaggle.com/datasets/kshitij192/cars-image-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xq_AKELpIGQ",
        "outputId": "db8f93e4-acf4-42fa-fa45-6bf089769e4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: s\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/kshitij192/cars-image-dataset\n",
            "Downloading cars-image-dataset.zip to ./cars-image-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36.3M/36.3M [00:00<00:00, 54.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AZR1kDy9fdZy"
      },
      "outputs": [],
      "source": [
        "# #Data of Toyota cars\n",
        "# opd.download(\"https://www.kaggle.com/datasets/occultainsights/toyota-cars-over-20k-labeled-images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u4RgLOb0HkC6"
      },
      "outputs": [],
      "source": [
        "# opd.download(\"https://www.kaggle.com/datasets/occultainsights/bmw-cars-over-11k-labeled-images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wQZ8zOik97JB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebda971-8ddc-4be9-9a1f-12abf530b25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0jvwOtYe-Wom"
      },
      "outputs": [],
      "source": [
        "#Now to retrieve the dataset and collect the information\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "class Place2_Data(data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        # location of the dataset\n",
        "        self.root = root\n",
        "        # list of classes\n",
        "        self.classes= (os.listdir(self.root))\n",
        "        self.classes_to_num= {c:i for i,c in enumerate(self.classes)}\n",
        "\n",
        "        # all images\n",
        "        self.all_images= [(os.path.join(root, c, img), self.classes_to_num[c]) for c in self.classes for img in os.listdir(os.path.join(root, c))]\n",
        "        # transforms\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image, classes = self.all_images[index]\n",
        "      image = Image.open(image).convert(mode=\"RGB\")\n",
        "      #img = Image.open(img_path).convert('RGB')\n",
        "      # apply transformation\n",
        "      if self.transforms:\n",
        "          image = self.transforms(image)\n",
        "      # return the image and class\n",
        "      return image, classes\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the total number of images\n",
        "        return len(self.all_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dnf2eVkQXG7C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "class ResNet50Multiclass(nn.Module):\n",
        "    def __init__(self, num_brand_classes, num_type_classes):\n",
        "        super(ResNet50Multiclass, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        # Remove the original fully connected layer\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "        in_features = 2048  # Output from resnet50 after the average pooling layer\n",
        "\n",
        "        # Define two fully connected layers for car brands and car types\n",
        "        self.fc_brand = nn.Linear(in_features, num_brand_classes)\n",
        "        self.fc_type = nn.Linear(in_features, num_type_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.model(x)\n",
        "        features = features.view(features.size(0), -1)  # Flatten the features\n",
        "        brand_output = self.fc_brand(features)\n",
        "        type_output = self.fc_type(features)\n",
        "        return brand_output, type_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchsummary as summary\n",
        "# resnet = ResNet50Multiclass(num_brand_classes=len(logoTrainData.classes), num_type_classes=len(carData.classes))\n",
        "# resnet.to(device)\n",
        "# summary.summary(resnet,input_size=(3,224,224))"
      ],
      "metadata": {
        "id": "beqYjyb5iprA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nk63zFxU-ZXz"
      },
      "outputs": [],
      "source": [
        "#Define the dataset for the car logos\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, RandomAffine, RandomRotation\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GoBiztIZ-Kye"
      },
      "outputs": [],
      "source": [
        "carImageData = \"/content/vehicle-type-recognition/Dataset\"\n",
        "carTrainLogoData = \"/content/car-brand-logos/Car_Brand_Logos/Train\"\n",
        "carTestLogoData = \"/content/car-brand-logos/Car_Brand_Logos/Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CtbfxAeV1MVV"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "logoTrainData = Place2_Data(carTrainLogoData, transforms=image_transform)\n",
        "logoTest = Place2_Data(carTestLogoData, transforms=image_transform)\n",
        "carData = Place2_Data(carImageData, transforms=image_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xUn47BMt3pno"
      },
      "outputs": [],
      "source": [
        "# Split datasets into training, validation, and test sets\n",
        "train_size = int(0.8 * len(carData))\n",
        "test_val_size = len(carData) - train_size\n",
        "train_data, test_val_data = random_split(carData, [train_size, test_val_size])\n",
        "\n",
        "val_size = int(0.5 * len(test_val_data))\n",
        "test_size = len(test_val_data) - val_size\n",
        "val_data, test_data = random_split(test_val_data, [val_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t0snxYZt3vz6"
      },
      "outputs": [],
      "source": [
        "# Create data loaders for both models\n",
        "train_loader_type = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader_type = DataLoader(val_data, batch_size=32, shuffle=True)\n",
        "test_loader_type = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_logo = DataLoader(logoTrainData, batch_size=32, shuffle=True)\n",
        "test_loader_logo = DataLoader(logoTest, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(carData.all_images)"
      ],
      "metadata": {
        "id": "yvZezghd2BVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2e0d65-eaf7-48af-ac3a-66d3f312e4f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnOoy2s42uoU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import plot library\n",
        "import matplotlib.pyplot as plt\n",
        "# iterate the dataloader\n",
        "_, (example_datas, labels) = next(enumerate(train_loader_logo))\n",
        "# get the first data\n",
        "sample = example_datas[1]\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.imshow(sample.permute(1, 2, 0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "tshYLT3NmLiD",
        "outputId": "aa7dfc33-3d89-4bd4-f1cb-5eeb51d2d86c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bf1d3fea9b0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T9TitAEr3zaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091a5ef2-c800-4449-cb83-f53403897498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 165MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define the multiclass model\n",
        "model_A = ResNet50Multiclass(num_brand_classes=len(logoTest.classes), num_type_classes=len(carData.classes))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_A = model_A.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_B = ResNet50Multiclass(num_brand_classes=len(logoTrainData.classes), num_type_classes=len(carData.classes))\n",
        "model_B = model_B.to(device)"
      ],
      "metadata": {
        "id": "Ttr4Yju1KY0A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_A = optim.Adam(model_A.parameters(), lr=0.0002)\n",
        "optimizer_B = optim.Adam(model_B.parameters(), lr=0.0002)\n"
      ],
      "metadata": {
        "id": "3-awU32gKVK-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjLaFnsDq6bp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for Model A (focus on car brands)\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model_A.train()\n",
        "    running_loss_A = 0.0\n",
        "\n",
        "    for images_brand, labels_brand in train_loader_logo:\n",
        "        images_brand, labels_brand = images_brand.to(device), labels_brand.to(device)\n",
        "\n",
        "        optimizer_A.zero_grad()\n",
        "\n",
        "        # Forward pass for Model A\n",
        "        brand_output_A, _ = model_A(images_brand)\n",
        "\n",
        "        # Calculate loss for Model A\n",
        "        loss_brand_A = criterion(brand_output_A, labels_brand)\n",
        "\n",
        "        loss_brand_A.backward()\n",
        "        optimizer_A.step()\n",
        "\n",
        "        running_loss_A += loss_brand_A.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss Model A: {running_loss_A/len(train_loader_logo)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHj-FDpSQDmI",
        "outputId": "ebd64200-6665-4d25-f5f9-50ad79af99cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss Model A: 0.5411279911670503\n",
            "Epoch 2/20 - Loss Model A: 0.22958336386216593\n",
            "Epoch 3/20 - Loss Model A: 0.1396992389772889\n",
            "Epoch 4/20 - Loss Model A: 0.13672796090994077\n",
            "Epoch 5/20 - Loss Model A: 0.10528416152383331\n",
            "Epoch 6/20 - Loss Model A: 0.0777480154286457\n",
            "Epoch 7/20 - Loss Model A: 0.083242902698738\n",
            "Epoch 8/20 - Loss Model A: 0.04190052135205014\n",
            "Epoch 9/20 - Loss Model A: 0.11674803831243204\n",
            "Epoch 10/20 - Loss Model A: 0.04127173554122778\n",
            "Epoch 11/20 - Loss Model A: 0.01773729246504652\n",
            "Epoch 12/20 - Loss Model A: 0.019085976241386342\n",
            "Epoch 13/20 - Loss Model A: 0.018060276931729402\n",
            "Epoch 14/20 - Loss Model A: 0.015497194920648832\n",
            "Epoch 15/20 - Loss Model A: 0.01183372186909465\n",
            "Epoch 16/20 - Loss Model A: 0.008651996234025147\n",
            "Epoch 17/20 - Loss Model A: 0.01881182526939702\n",
            "Epoch 18/20 - Loss Model A: 0.029408022519304192\n",
            "Epoch 19/20 - Loss Model A: 0.08727893292538444\n",
            "Epoch 20/20 - Loss Model A: 0.10043708230899293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for Model B (focus on car types)\n",
        "for epoch in range(num_epochs):\n",
        "    model_B.train()\n",
        "    running_loss_B = 0.0\n",
        "\n",
        "    for images_type, labels_type in train_loader_type:\n",
        "        images_type, labels_type = images_type.to(device), labels_type.to(device)\n",
        "\n",
        "        optimizer_B.zero_grad()\n",
        "\n",
        "        # Forward pass for Model B\n",
        "        _, type_output_B = model_B(images_type)\n",
        "\n",
        "        # Calculate loss for Model B\n",
        "        loss_type_B = criterion(type_output_B, labels_type)\n",
        "\n",
        "        loss_type_B.backward()\n",
        "        optimizer_B.step()\n",
        "\n",
        "        running_loss_B += loss_type_B.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss Model B: {running_loss_B/len(train_loader_type)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHwdxY_JP5YL",
        "outputId": "622aa46e-0eab-4101-d61f-341e4e3b1f59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss Model B: 0.5209751173853874\n",
            "Epoch 2/20 - Loss Model B: 0.09701013136655093\n",
            "Epoch 3/20 - Loss Model B: 0.04764175214804709\n",
            "Epoch 4/20 - Loss Model B: 0.03984006460523233\n",
            "Epoch 5/20 - Loss Model B: 0.01318682029377669\n",
            "Epoch 6/20 - Loss Model B: 0.028398978372570126\n",
            "Epoch 7/20 - Loss Model B: 0.00817385276313871\n",
            "Epoch 8/20 - Loss Model B: 0.014136527443770319\n",
            "Epoch 9/20 - Loss Model B: 0.0014407838374609129\n",
            "Epoch 10/20 - Loss Model B: 0.001523553443257697\n",
            "Epoch 11/20 - Loss Model B: 0.0023249295802088453\n",
            "Epoch 12/20 - Loss Model B: 0.0019244461902417243\n",
            "Epoch 13/20 - Loss Model B: 0.0010162173333810643\n",
            "Epoch 14/20 - Loss Model B: 0.10967753491713665\n",
            "Epoch 15/20 - Loss Model B: 0.1390762772411108\n",
            "Epoch 16/20 - Loss Model B: 0.07667491752654314\n",
            "Epoch 17/20 - Loss Model B: 0.03450892218388617\n",
            "Epoch 18/20 - Loss Model B: 0.021188357938081027\n",
            "Epoch 19/20 - Loss Model B: 0.012174641236197204\n",
            "Epoch 20/20 - Loss Model B: 0.02627146309823729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def test_model(model, test_loader, device, task='type'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)[0] if task == 'brand' else model(inputs)[1]\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy ({task}): {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "XGSSpexOK3PK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nTesting Model A (car brands focus)...\")\n",
        "test_model(model_A, test_loader_logo, device, task='brand')  # Test Model A on car brands\n",
        "test_model(model_A, test_loader_type, device, task='type')  # Test Model A on car types\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3sZ3XpnQQp0",
        "outputId": "6f38d484-c587-4c9f-c91c-9db3eae3a466"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Model A (car brands focus)...\n",
            "Test Accuracy (brand): 87.00%\n",
            "Test Accuracy (type): 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nTesting Model B (car types focus)...\")\n",
        "test_model(model_B, test_loader_logo, device, task='brand')  # Test Model B on car brands\n",
        "test_model(model_B, test_loader_type, device, task='type')  # Test Model B on car types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhui7_6yQR-R",
        "outputId": "8493f1e7-3b33-4e09-8cff-f01ea43a5a51"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Model B (car types focus)...\n",
            "Test Accuracy (brand): 9.00%\n",
            "Test Accuracy (type): 85.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_model_A = model_A\n",
        "f_model_A = model_A\n",
        "m_model_B = model_B\n",
        "f_model_B = model_B"
      ],
      "metadata": {
        "id": "K0o3GcikD8P3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The Mutual Learning Process"
      ],
      "metadata": {
        "id": "yU2BOKuote-B"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutual learning loss function\n",
        "def mutual_learning_loss(output_A, output_B, target, T=2):\n",
        "    \"\"\"\n",
        "    Calculate the mutual learning loss, which is the Kullback-Leibler divergence between the\n",
        "    softmax outputs of model_A and model_B.\n",
        "    T is the temperature parameter for softening the logits.\n",
        "    \"\"\"\n",
        "    soft_A = nn.functional.log_softmax(output_A / T, dim=1)\n",
        "    soft_B = nn.functional.softmax(output_B / T, dim=1)\n",
        "    loss = nn.functional.kl_div(soft_A, soft_B, reduction='batchmean') * (T * T)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "lkM4em7VoMRx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with mutual learning\n",
        "num_epochs = 20\n",
        "lambda_ = 0.5  # Weight for mutual learning loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    m_model_A.train()\n",
        "\n",
        "    running_loss_A = 0.0\n",
        "\n",
        "    # Model A focuses more on car brands, but also learns from car types\n",
        "    for (images_brand, labels_brand), (images_type, labels_type) in zip(train_loader_logo, train_loader_type):\n",
        "        images_brand, labels_brand = images_brand.to(device), labels_brand.to(device)\n",
        "        images_type, labels_type = images_type.to(device), labels_type.to(device)\n",
        "\n",
        "        optimizer_A.zero_grad()\n",
        "\n",
        "        # Forward pass for Model A\n",
        "        brand_output_A, type_output_A = m_model_A(images_brand)\n",
        "        type_output_A_2 = m_model_A(images_type)[1]  # Get type output for car types data\n",
        "\n",
        "        # Calculate losses for Model A\n",
        "        loss_brand_A = criterion(brand_output_A, labels_brand)\n",
        "        loss_type_A = criterion(type_output_A_2, labels_type)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            brand_output_B, type_output_B = model_B(images_type)\n",
        "\n",
        "        # Mutual learning loss\n",
        "        mutual_loss_A = -torch.sum(torch.softmax(brand_output_B, dim=1) * torch.log_softmax(brand_output_A, dim=1), dim=1).mean()\n",
        "        mutual_loss_A += -torch.sum(torch.softmax(type_output_B, dim=1) * torch.log_softmax(type_output_A_2, dim=1), dim=1).mean()\n",
        "\n",
        "        # Total loss for Model A\n",
        "        total_loss_A = loss_brand_A + 0.5 * loss_type_A + lambda_ * mutual_loss_A\n",
        "        total_loss_A.backward()\n",
        "        optimizer_A.step()\n",
        "\n",
        "        running_loss_A += total_loss_A.item()\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss Model A: {running_loss_A/len(train_loader_type)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ3FlEf8KohW",
        "outputId": "5c3963a4-7fa8-41d1-ff00-16273f8549b5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss Model A: 5.026371264457703\n",
            "Epoch 2/20 - Loss Model A: 2.91513352394104\n",
            "Epoch 3/20 - Loss Model A: 2.2428626298904417\n",
            "Epoch 4/20 - Loss Model A: 2.0343040585517884\n",
            "Epoch 5/20 - Loss Model A: 1.9160762548446655\n",
            "Epoch 6/20 - Loss Model A: 1.9003225207328795\n",
            "Epoch 7/20 - Loss Model A: 1.8379117369651794\n",
            "Epoch 8/20 - Loss Model A: 1.8443563580513\n",
            "Epoch 9/20 - Loss Model A: 1.832475483417511\n",
            "Epoch 10/20 - Loss Model A: 1.8221534848213197\n",
            "Epoch 11/20 - Loss Model A: 1.8034896016120912\n",
            "Epoch 12/20 - Loss Model A: 1.7944860339164734\n",
            "Epoch 13/20 - Loss Model A: 1.796592104434967\n",
            "Epoch 14/20 - Loss Model A: 1.7946195960044862\n",
            "Epoch 15/20 - Loss Model A: 1.7858216285705566\n",
            "Epoch 16/20 - Loss Model A: 1.787221646308899\n",
            "Epoch 17/20 - Loss Model A: 1.7758725881576538\n",
            "Epoch 18/20 - Loss Model A: 1.7915038228034974\n",
            "Epoch 19/20 - Loss Model A: 1.7984546422958374\n",
            "Epoch 20/20 - Loss Model A: 1.7783400893211365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  m_model_B.train()\n",
        "\n",
        "  running_loss_B = 0.0\n",
        "  # Model B focuses more on car types, but also learns from car brands\n",
        "  for (images_type, labels_type), (images_brand, labels_brand) in zip(train_loader_type, train_loader_logo):\n",
        "      images_type, labels_type = images_type.to(device), labels_type.to(device)\n",
        "      images_brand, labels_brand = images_brand.to(device), labels_brand.to(device)\n",
        "\n",
        "      optimizer_B.zero_grad()\n",
        "\n",
        "      # Forward pass for Model B\n",
        "      brand_output_B, type_output_B = m_model_B(images_brand)\n",
        "      type_output_B_2 = m_model_B(images_type)[1]  # Get type output for car types data\n",
        "\n",
        "      # Calculate losses for Model B\n",
        "      loss_type_B = criterion(type_output_B_2, labels_type)\n",
        "      loss_brand_B = criterion(brand_output_B, labels_brand)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          brand_output_A, type_output_A = model_A(images_brand)\n",
        "\n",
        "      # Mutual learning loss\n",
        "      mutual_loss_B = -torch.sum(torch.softmax(brand_output_A, dim=1) * torch.log_softmax(brand_output_B, dim=1), dim=1).mean()\n",
        "      mutual_loss_B += -torch.sum(torch.softmax(type_output_A, dim=1) * torch.log_softmax(type_output_B_2, dim=1), dim=1).mean()\n",
        "\n",
        "      # Total loss for Model B\n",
        "      total_loss_B = loss_type_B + 0.5 * loss_brand_B + lambda_ * mutual_loss_B\n",
        "      total_loss_B.backward()\n",
        "      optimizer_B.step()\n",
        "\n",
        "      running_loss_B += total_loss_B.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs} - Loss Model B: {running_loss_B/len(train_loader_type)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rBr2fAvL-fA",
        "outputId": "1d8a13f4-34a9-4972-9ba3-576ada75b43e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Loss Model B: 4.6398731708526615\n",
            "Epoch 2/20 - Loss Model B: 3.3561201095581055\n",
            "Epoch 3/20 - Loss Model B: 2.7599645137786863\n",
            "Epoch 4/20 - Loss Model B: 2.5331584930419924\n",
            "Epoch 5/20 - Loss Model B: 2.3104799032211303\n",
            "Epoch 6/20 - Loss Model B: 2.3304309129714964\n",
            "Epoch 7/20 - Loss Model B: 2.2734689712524414\n",
            "Epoch 8/20 - Loss Model B: 2.2846449613571167\n",
            "Epoch 9/20 - Loss Model B: 2.233642339706421\n",
            "Epoch 10/20 - Loss Model B: 2.179830527305603\n",
            "Epoch 11/20 - Loss Model B: 2.169286918640137\n",
            "Epoch 12/20 - Loss Model B: 2.1491469621658323\n",
            "Epoch 13/20 - Loss Model B: 2.192752242088318\n",
            "Epoch 14/20 - Loss Model B: 2.208958387374878\n",
            "Epoch 15/20 - Loss Model B: 2.1352848291397093\n",
            "Epoch 16/20 - Loss Model B: 2.16834819316864\n",
            "Epoch 17/20 - Loss Model B: 2.132338523864746\n",
            "Epoch 18/20 - Loss Model B: 2.1471558213233948\n",
            "Epoch 19/20 - Loss Model B: 2.1208825588226317\n",
            "Epoch 20/20 - Loss Model B: 2.1240820050239564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the models\n",
        "test_model(m_model_A, test_loader_logo, device, task='brand')  # Test Model A on car brands\n",
        "test_model(m_model_A, test_loader_type, device, task='type')  # Test Model A on car types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNPUbno4LLho",
        "outputId": "27683a6d-f548-471e-b8a6-06926b8d1ddf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (brand): 88.00%\n",
            "Test Accuracy (type): 70.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(m_model_B, test_loader_logo, device, task='brand')  # Test Model B on car brands\n",
        "test_model(m_model_B, test_loader_type, device, task='type')  # Test Model B on car types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQhfAsqANCSo",
        "outputId": "3d790a01-dfa8-4d6c-eafa-8d3990603188"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (brand): 85.00%\n",
            "Test Accuracy (type): 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mmoKo3fBJCE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature level Model fusion\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, num_brand_classes, num_type_classes):\n",
        "        super(FusionModel, self).__init__()\n",
        "        # Assuming the output from both models has the same size, i.e., 2048 for ResNet50\n",
        "        self.fc_fusion_brand = nn.Linear(2 * num_brand_classes, num_brand_classes)\n",
        "        self.fc_fusion_type = nn.Linear(2 * num_type_classes, num_type_classes)\n",
        "\n",
        "    def forward(self, brand_output_A, brand_output_B, type_output_A, type_output_B):\n",
        "        # Concatenate outputs from Model A and Model B\n",
        "        fusion_brand = torch.cat((brand_output_A, brand_output_B), dim=1)\n",
        "        fusion_type = torch.cat((type_output_A, type_output_B), dim=1)\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        fusion_brand_output = self.fc_fusion_brand(fusion_brand)\n",
        "        fusion_type_output = self.fc_fusion_type(fusion_type)\n",
        "\n",
        "        return fusion_brand_output, fusion_type_output"
      ],
      "metadata": {
        "id": "xL14WyW8OzUQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the fusion model\n",
        "fusion_model = FusionModel(num_brand_classes=len(logoTrainData.classes), num_type_classes=len(carData.classes))\n",
        "fusion_model = fusion_model.to(device)\n",
        "\n",
        "# Define optimizer and loss function for the fusion model\n",
        "optimizer_fusion = optim.Adam(fusion_model.parameters(), lr=0.0001)\n",
        "criterion_fusion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop for the fusion model\n",
        "for epoch in range(num_epochs):\n",
        "    fusion_model.train()\n",
        "    running_loss_fusion = 0.0\n",
        "\n",
        "    # Loop through both datasets\n",
        "    for (images_brand, labels_brand), (images_type, labels_type) in zip(train_loader_logo, train_loader_type):\n",
        "        images_brand, labels_brand = images_brand.to(device), labels_brand.to(device)\n",
        "        images_type, labels_type = images_type.to(device), labels_type.to(device)\n",
        "\n",
        "        optimizer_fusion.zero_grad()\n",
        "\n",
        "        # Forward pass through Model A and Model B\n",
        "        brand_output_A, type_output_A = model_A(images_brand)\n",
        "        brand_output_B, type_output_B = model_B(images_type)\n",
        "\n",
        "        # Forward pass through the fusion model\n",
        "        fusion_brand_output, fusion_type_output = fusion_model(brand_output_A, brand_output_B, type_output_A, type_output_B)\n",
        "\n",
        "        # Calculate fusion loss\n",
        "        loss_fusion_brand = criterion_fusion(fusion_brand_output, labels_brand)\n",
        "        loss_fusion_type = criterion_fusion(fusion_type_output, labels_type)\n",
        "\n",
        "        total_loss_fusion = loss_fusion_brand + loss_fusion_type\n",
        "\n",
        "        total_loss_fusion.backward()\n",
        "        optimizer_fusion.step()\n",
        "\n",
        "        running_loss_fusion += total_loss_fusion.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Fusion Model Loss: {running_loss_fusion/len(train_loader_logo)+len(train_loader_type)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj_1mWPwxivv",
        "outputId": "0dbe7a70-6e88-47cf-a0c2-8e9f891ac16e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Fusion Model Loss: 10.47740428960776\n",
            "Epoch 2/20 - Fusion Model Loss: 10.48310658599757\n",
            "Epoch 3/20 - Fusion Model Loss: 10.480366613291487\n",
            "Epoch 4/20 - Fusion Model Loss: 10.48455049418196\n",
            "Epoch 5/20 - Fusion Model Loss: 10.464703846581374\n",
            "Epoch 6/20 - Fusion Model Loss: 10.470106734505183\n",
            "Epoch 7/20 - Fusion Model Loss: 10.476882925516442\n",
            "Epoch 8/20 - Fusion Model Loss: 10.480089996434465\n",
            "Epoch 9/20 - Fusion Model Loss: 10.469051883190494\n",
            "Epoch 10/20 - Fusion Model Loss: 10.468345614928234\n",
            "Epoch 11/20 - Fusion Model Loss: 10.464137164852287\n",
            "Epoch 12/20 - Fusion Model Loss: 10.4691123358811\n",
            "Epoch 13/20 - Fusion Model Loss: 10.465304145330116\n",
            "Epoch 14/20 - Fusion Model Loss: 10.457782283613954\n",
            "Epoch 15/20 - Fusion Model Loss: 10.463456479808952\n",
            "Epoch 16/20 - Fusion Model Loss: 10.46818857555148\n",
            "Epoch 17/20 - Fusion Model Loss: 10.454742268670964\n",
            "Epoch 18/20 - Fusion Model Loss: 10.459467414059217\n",
            "Epoch 19/20 - Fusion Model Loss: 10.452131893061384\n",
            "Epoch 20/20 - Fusion Model Loss: 10.45429568049274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_fusion_model(model_A, model_B, fusion_model, test_loader, device, task='brand'):\n",
        "    model_A.eval()\n",
        "    model_B.eval()\n",
        "    fusion_model.eval()\n",
        "    model_A.to(device)\n",
        "    model_B.to(device)\n",
        "    fusion_model.to(device)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Get outputs from both Model A and Model B\n",
        "            brand_output_A, type_output_A = model_A(inputs)\n",
        "            brand_output_B, type_output_B = model_B(inputs)\n",
        "\n",
        "            # Get the fusion model's output\n",
        "            if task == 'brand':\n",
        "                outputs = fusion_model(brand_output_A, brand_output_B, type_output_A, type_output_B)[0]\n",
        "            else:\n",
        "                outputs = fusion_model(brand_output_A, brand_output_B, type_output_A, type_output_B)[1]\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy (Fusion Model - {task}): {accuracy:.2f}%')\n",
        "\n",
        "# Testing the fusion model on car brands and car types\n",
        "print(\"\\nTesting Fusion Model...\")\n",
        "test_fusion_model(f_model_A, f_model_B, fusion_model, test_loader_logo, device, task='brand')\n",
        "test_fusion_model(f_model_A, f_model_B, fusion_model, test_loader_type, device, task='type')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in-TmQxP58Fp",
        "outputId": "a22d8ee6-234d-40db-8c39-fd9083fd840b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Fusion Model...\n",
            "Test Accuracy (Fusion Model - brand): 13.75%\n",
            "Test Accuracy (Fusion Model - type): 40.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FusionModels(nn.Module):\n",
        "    def __init__(self, model_A, model_B):\n",
        "        super(FusionModels, self).__init__()  # Corrected to use the current class name\n",
        "        self.model_A = model_A\n",
        "        self.model_B = model_B\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through both models\n",
        "        brand_output_A, type_output_A = self.model_A(x)\n",
        "        brand_output_B, type_output_B = self.model_B(x)\n",
        "\n",
        "        # Averaging the outputs for fusion\n",
        "        fused_brand_output = (brand_output_A + brand_output_B) / 2\n",
        "        fused_type_output = (type_output_A + type_output_B) / 2\n",
        "\n",
        "        return fused_brand_output, fused_type_output\n"
      ],
      "metadata": {
        "id": "2faBjcUn60Mi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming model_A and model_B are already trained and available\n",
        "fusion_model = FusionModels(f_model_A, f_model_B).to(device)\n",
        "\n",
        "fusion_model.eval()  # Set the fusion model to evaluation mode\n",
        "\n",
        "correct_brand_fusion = 0\n",
        "total_brand_fusion = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for testing\n",
        "    for images_brand, labels_brand in test_loader_logo:  # Test loader for car brands\n",
        "        images_brand, labels_brand = images_brand.to(device), labels_brand.to(device)\n",
        "\n",
        "        # Forward pass through the fusion model\n",
        "        fused_brand_output, _ = fusion_model(images_brand)\n",
        "\n",
        "        # Get predictions for car brands\n",
        "        _, predicted_brand_fusion = torch.max(fused_brand_output, 1)\n",
        "\n",
        "        # Update correct and total counts for brand prediction\n",
        "        total_brand_fusion += labels_brand.size(0)\n",
        "        correct_brand_fusion += (predicted_brand_fusion == labels_brand).sum().item()\n",
        "\n",
        "# Calculate accuracy for the fusion model on car brand classification\n",
        "accuracy_brand_fusion = 100 * correct_brand_fusion / total_brand_fusion\n",
        "print(f'Accuracy of Fusion Model on Car Brand Classification: {accuracy_brand_fusion:.2f}%')"
      ],
      "metadata": {
        "id": "2N8DcPtz5r4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304bf4c8-138d-4628-91d9-c0ba8171cce8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fusion Model on Car Brand Classification: 88.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_type_fusion = 0\n",
        "total_type_fusion = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for testing\n",
        "    for images_type, labels_type in test_loader_type:  # Test loader for car types\n",
        "        images_type, labels_type = images_type.to(device), labels_type.to(device)\n",
        "\n",
        "        # Forward pass through the fusion model\n",
        "        _, fused_type_output = fusion_model(images_type)\n",
        "\n",
        "        # Get predictions for car types\n",
        "        _, predicted_type_fusion = torch.max(fused_type_output, 1)\n",
        "\n",
        "        # Update correct and total counts for type prediction\n",
        "        total_type_fusion += labels_type.size(0)\n",
        "        correct_type_fusion += (predicted_type_fusion == labels_type).sum().item()\n",
        "\n",
        "# Calculate accuracy for the fusion model on car type classification\n",
        "accuracy_type_fusion = 100 * correct_type_fusion / total_type_fusion\n",
        "print(f'Accuracy of Fusion Model on Car Type Classification: {accuracy_type_fusion:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-sxIvDv6_Om",
        "outputId": "afd863fb-7b54-4071-a612-0b68e98b45f2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fusion Model on Car Type Classification: 77.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_uKGSxLN8ofI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}